@INPROCEEDINGS{LayoutLLM,
  author={Luo, Chuwei and Shen, Yufan and Zhu, Zhaoqing and Zheng, Qi and Yu, Zhi and Yao, Cong},
  booktitle={2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={{LayoutLLM: Layout Instruction Tuning with Large Language Models for Document Understanding}}, 
  year={2024},
  volume={},
  number={},
  pages={15630-15640},
  keywords={Computer vision;Large language models;Layout;Manuals;Inspection;Benchmark testing;Boosting;Document Understanding;Layout;Large Language Models},
  doi={10.1109/CVPR52733.2024.01480}}

@article{DocKylin,
  author       = {Jiaxin Zhang and
                  Wentao Yang and
                  Songxuan Lai and
                  Zecheng Xie and
                  Lianwen Jin},
  title        = {{DocKylin: A Large Multimodal Model for Visual Document Understanding
                  with Efficient Visual Slimming}},
  journal      = {CoRR},
  volume       = {abs/2406.19101},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2406.19101},
  doi          = {10.48550/ARXIV.2406.19101},
  eprinttype    = {arXiv},
  eprint       = {2406.19101},
  timestamp    = {Mon, 04 Nov 2024 22:29:59 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2406-19101.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{GenKIE,
  author       = {Panfeng Cao and
                  Ye Wang and
                  Qiang Zhang and
                  Zaiqiao Meng},
  editor       = {Houda Bouamor and
                  Juan Pino and
                  Kalika Bali},
  title        = {{GenKIE: Robust Generative Multimodal Document Key Information Extraction}},
  booktitle    = {Findings of the Association for Computational Linguistics: {EMNLP}
                  2023},
  pages        = {14702--14713},
  publisher    = {Association for Computational Linguistics},
  year         = {2023},
  url          = {https://doi.org/10.18653/v1/2023.findings-emnlp.979},
  doi          = {10.18653/V1/2023.FINDINGS-EMNLP.979},
  timestamp    = {Wed, 17 Jul 2024 16:21:24 +0200},
  biburl       = {https://dblp.org/rec/conf/emnlp/CaoWZM23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{WebTable,
  author       = {Larissa R. Lautert and
                  Marcelo M. Scheidt and
                  Carina F. Dorneles},
  title        = {{Web table taxonomy and formalization}},
  journal      = {{SIGMOD} Rec.},
  volume       = {42},
  number       = {3},
  pages        = {28--33},
  year         = {2013},
  url          = {https://doi.org/10.1145/2536669.2536674},
  doi          = {10.1145/2536669.2536674},
  timestamp    = {Thu, 14 Oct 2021 09:11:37 +0200},
  biburl       = {https://dblp.org/rec/journals/sigmod/LautertSD13.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{HeYM0D0L024,
    title = "{W}eb{V}oyager: Building an End-to-End Web Agent with Large Multimodal Models",
    author = "He, Hongliang  and
      Yao, Wenlin  and
      Ma, Kaixin  and
      Yu, Wenhao  and
      Dai, Yong  and
      Zhang, Hongming  and
      Lan, Zhenzhong  and
      Yu, Dong",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.371/",
    doi = "10.18653/v1/2024.acl-long.371",
    pages = "6864--6890",
    abstract = "The rapid advancement of large language models (LLMs) has led to a new era marked by the development of autonomous applications in real-world scenarios, which drives innovation in creating advanced web agents. Existing web agents typically only handle one input modality and are evaluated only in simplified web simulators or static web snapshots, greatly limiting their applicability in real-world scenarios. To bridge this gap, we introduce WebVoyager, an innovative Large Multimodal Model (LMM) powered web agent that can complete user instructions end-to-end by interacting with real-world websites. Moreover, we establish a new benchmark by compiling real-world tasks from 15 popular websites and introduce an automatic evaluation protocol leveraging multimodal understanding abilities of GPT-4V to evaluate open-ended web agents. We show that WebVoyager achieves a 59.1{\%} task success rate on our benchmark, significantly surpassing the performance of both GPT-4 (All Tools) and the WebVoyager (text-only) setups, underscoring the exceptional capability of WebVoyager. The proposed automatic evaluation metric achieves 85.3{\%} agreement with human judgment, indicating its effectiveness in providing reliable and accurate assessments of web agents."
}


@article{tableasimages24,
  author       = {Naihao Deng and
                  Zhenjie Sun and
                  Ruiqi He and
                  Aman Sikka and
                  Yulong Chen and
                  Lin Ma and
                  Yue Zhang and
                  Rada Mihalcea},
  title        = {{Tables as Images? Exploring the Strengths and Limitations of LLMs
                  on Multimodal Representations of Tabular Data}},
  journal      = {CoRR},
  volume       = {abs/2402.12424},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2402.12424},
  doi          = {10.48550/ARXIV.2402.12424},
  eprinttype    = {arXiv},
  eprint       = {2402.12424},
  timestamp    = {Tue, 14 Jan 2025 08:02:44 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2402-12424.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{ZhengGK0024,
  author       = {Boyuan Zheng and
                  Boyu Gou and
                  Jihyung Kil and
                  Huan Sun and
                  Yu Su},
  title        = {{GPT-4V(ision) is a Generalist Web Agent, if Grounded}},
  booktitle    = {Forty-first International Conference on Machine Learning, {ICML} 2024,
                  Vienna, Austria},
  year         = {2024},
  url          = {https://openreview.net/forum?id=piecKJ2DlB},
  timestamp    = {Mon, 02 Sep 2024 16:55:26 +0200},
  biburl       = {https://dblp.org/rec/conf/icml/ZhengGK0024.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{AntolALMBZP15,
  author       = {Stanislaw Antol and
                  Aishwarya Agrawal and
                  Jiasen Lu and
                  Margaret Mitchell and
                  Dhruv Batra and
                  C. Lawrence Zitnick and
                  Devi Parikh},
  title        = {{VQA: Visual Question Answering}},
  journal      = {CoRR},
  volume       = {abs/1505.00468},
  year         = {2015},
  url          = {http://arxiv.org/abs/1505.00468},
  eprinttype    = {arXiv},
  eprint       = {1505.00468},
  timestamp    = {Mon, 13 Aug 2018 16:48:30 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/AntolALMBZP15.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{SuiZZH024,
author = {Sui, Yuan and Zhou, Mengyu and Zhou, Mingjie and Han, Shi and Zhang, Dongmei},
title = {Table Meets LLM: Can Large Language Models Understand Structured Table Data? A Benchmark and Empirical Study},
booktitle = {The 17th ACM International Conference on Web Search and Data Mining (WSDM '24) Mérida, Mexico},
year = {2024},
month = {March},
abstract = {Large language models (LLMs) are becoming attractive as few-shot reasoners to solve Natural Language (NL)-related tasks. However, there is still much to learn about how well LLMs understand structured data, such as tables. Although tables can be used as input to LLMs with serialization, there is a lack of comprehensive studies that examine whether LLMs can truly comprehend such data. In this paper, we try to understand this by designing a benchmark to evaluate the structural understanding capabilities (SUC) of LLMs. The benchmark we create includes seven tasks, each with its own unique challenges, e.g., cell lookup, row retrieval, and size detection. We perform a series of evaluations on GPT-3.5 and GPT-4. We find that performance varied depending on several input choices, including table input format, content order, role prompting, and partition marks. Drawing from the insights gained through the benchmark evaluations, we propose self-augmentation for effective structural prompting, such as critical value / range identification using internal knowledge of LLMs. When combined with carefully chosen input choices, these structural prompting methods lead to promising improvements in LLM performance on a variety of tabular tasks, e.g., TabFact (↑ 2.31%), HybridQA (↑ 2.13%), SQA (↑ 2.72%), Feverous (↑ 0.84%), and ToTTo (↑ 5.68%). We believe that our open source benchmark and proposed prompting methods can serve as a simple yet generic selection for future research.},
url = {https://www.microsoft.com/en-us/research/publication/table-meets-llm-can-large-language-models-understand-structured-table-data-a-benchmark-and-empirical-study/},
}

@inproceedings{ZhuLHWZLFC20,
    title = "{TAT}-{QA}: A Question Answering Benchmark on a Hybrid of Tabular and Textual Content in Finance",
    author = "Zhu, Fengbin  and
      Lei, Wenqiang  and
      Huang, Youcheng  and
      Wang, Chao  and
      Zhang, Shuo  and
      Lv, Jiancheng  and
      Feng, Fuli  and
      Chua, Tat-Seng",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.254/",
    doi = "10.18653/v1/2021.acl-long.254",
    pages = "3277--3287",
    abstract = "Hybrid data combining both tabular and textual content (e.g., financial reports) are quite pervasive in the real world. However, Question Answering (QA) over such hybrid data is largely neglected in existing research. In this work, we extract samples from real financial reports to build a new large-scale QA dataset containing both Tabular And Textual data, named TAT-QA, where numerical reasoning is usually required to infer the answer, such as addition, subtraction, multiplication, division, counting, comparison/sorting, and the compositions. We further propose a novel QA model termed TAGOP, which is capable of reasoning over both tables and text. It adopts sequence tagging to extract relevant cells from the table along with relevant spans from the text to infer their semantics, and then applies symbolic reasoning over them with a set of aggregation operators to arrive at the final answer. TAGOP achieves 58.0{\%} inF1, which is an 11.1{\%} absolute increase over the previous best baseline model, according to our experiments on TAT-QA. But this result still lags far behind performance of expert human, i.e.90.8{\%} in F1. It is demonstrated that our TAT-QA is very challenging and can serve as a benchmark for training and testing powerful QA models that address hybrid form data."
}



@article{BIRD2023,
  author       = {Jinyang Li and
                  Binyuan Hui and
                  Ge Qu and
                  Binhua Li and
                  Jiaxi Yang and
                  Bowen Li and
                  Bailin Wang and
                  Bowen Qin and
                  Rongyu Cao and
                  Ruiying Geng and
                  Nan Huo and
                  Xuanhe Zhou and
                  Chenhao Ma and
                  Guoliang Li and
                  Kevin Chen{-}Chuan Chang and
                  Fei Huang and
                  Reynold Cheng and
                  Yongbin Li},
  title        = {{Can LLM Already Serve as A Database Interface? A BIg Bench for
                  Large-Scale Database Grounded Text-to-SQLs}},
  journal      = {CoRR},
  volume       = {abs/2305.03111},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2305.03111},
  doi          = {10.48550/ARXIV.2305.03111},
  eprinttype    = {arXiv},
  eprint       = {2305.03111},
  timestamp    = {Tue, 11 Feb 2025 07:47:08 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2305-03111.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{spinach24,
    title = "{SPINACH}: {SPARQL}-Based Information Navigation for Challenging Real-World Questions",
    author = "Liu, Shicheng  and
      Semnani, Sina  and
      Triedman, Harold  and
      Xu, Jialiang  and
      Zhao, Isaac Dan  and
      Lam, Monica",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-emnlp.938/",
    doi = "10.18653/v1/2024.findings-emnlp.938",
    pages = "15977--16001",
    abstract = "Large Language Models (LLMs) have led to significant improvements in the Knowledge Base Question Answering (KBQA) task. However, datasets used in KBQA studies do not capture the true complexity of KBQA tasks. They either have simple questions, use synthetically generated logical forms, or are based on small knowledge base (KB) schemas.We introduce the SPINACH dataset, an expert-annotated KBQA dataset collected from discussions on Wikidata`s {\textquotedblleft}Request a Query{\textquotedblright} forum with 320 decontextualized question-SPARQL pairs. The complexity of these in-the-wild queries calls for a KBQA system that can dynamically explore large and often incomplete schemas and reason about them, as it is infeasible to create a comprehensive training dataset. We also introduce an in-context learning KBQA agent, also called SPINACH, that mimics how a human expert would write SPARQLs to handle challenging questions. SPINACH achieves a new state of the art on the QALD-7, QALD-9 Plus and QALD-10 datasets by 31.0{\%}, 27.0{\%}, and 10.0{\%} in $F_1$, respectively, and coming within 1.6{\%} of the fine-tuned LLaMA SOTA model on WikiWebQuestions.On our new SPINACH dataset, the SPINACH agent outperforms all baselines, including the best GPT-4-based KBQA agent, by at least 38.1{\%} in $F_1$."
}

@inproceedings{Atis,
    title = "Expanding the Scope of the {ATIS} Task: The {ATIS}-3 Corpus",
    author = "Dahl, Deborah A.  and
      Bates, Madeleine  and
      Brown, Michael  and
      Fisher, William  and
      Hunicke-Smith, Kate  and
      Pallett, David  and
      Pao, Christine  and
      Rudnicky, Alexander  and
      Shriberg, Elizabeth",
    booktitle = "{H}uman {L}anguage {T}echnology: Proceedings of a Workshop held at {P}lainsboro",
    year = "1994",
    url = "https://aclanthology.org/H94-1010/"
}

@inproceedings{sqa17,
    title = "Search-based Neural Structured Learning for Sequential Question Answering",
    author = "Iyyer, Mohit  and
      Yih, Wen-tau  and
      Chang, Ming-Wei",
    editor = "Barzilay, Regina  and
      Kan, Min-Yen",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P17-1167/",
    doi = "10.18653/v1/P17-1167",
    pages = "1821--1831",
    abstract = "Recent work in semantic parsing for question answering has focused on long and complicated questions, many of which would seem unnatural if asked in a normal conversation between two humans. In an effort to explore a conversational QA setting, we present a more realistic task: answering sequences of simple but inter-related questions. We collect a dataset of 6,066 question sequences that inquire about semi-structured tables from Wikipedia, with 17,553 question-answer pairs in total. To solve this sequential question answering task, we propose a novel dynamic neural semantic parsing framework trained using a weakly supervised reward-guided search. Our model effectively leverages the sequential context to outperform state-of-the-art QA systems that are designed to answer highly complex questions."
}

@inproceedings{QFMTS24,
  title={{QFMTS: Generating Query-Focused Summaries over Multi-Table Inputs}},
  author={Weijia Zhang and Vaishali Pal and Jia-Hong Huang and E. Kanoulas and Maarten de Rijke},
  booktitle={European Conference on Artificial Intelligence},
  year={2024},
  url={https://api.semanticscholar.org/CorpusID:269626608}
}

@article{MiMoTable,
  author       = {Zheng Li and
                  Yang Du and
                  Mao Zheng and
                  Mingyang Song},
  title        = {{MiMoTable: A Multi-scale Spreadsheet Benchmark with Meta Operations
                  for Table Reasoning}},
  journal      = {CoRR},
  volume       = {abs/2412.11711},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2412.11711},
  doi          = {10.48550/ARXIV.2412.11711},
  eprinttype    = {arXiv},
  eprint       = {2412.11711},
  timestamp    = {Tue, 21 Jan 2025 17:15:37 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2412-11711.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{multimodaltable24,
  author       = {Mingyu Zheng and
                  Xinwei Feng and
                  Qingyi Si and
                  Qiaoqiao She and
                  Zheng Lin and
                  Wenbin Jiang and
                  Weiping Wang},
  editor       = {Lun{-}Wei Ku and
                  Andre Martins and
                  Vivek Srikumar},
  title        = {{Multimodal Table Understanding}},
  booktitle    = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics},
  pages        = {9102--9124},
  publisher    = {Association for Computational Linguistics},
  year         = {2024},
  url          = {https://doi.org/10.18653/v1/2024.acl-long.493},
  doi          = {10.18653/V1/2024.ACL-LONG.493},
  timestamp    = {Mon, 10 Feb 2025 14:42:45 +0100},
  biburl       = {https://dblp.org/rec/conf/acl/ZhengFSS0J024.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{WTQ,
    title = "Compositional Semantic Parsing on Semi-Structured Tables",
    author = "Pasupat, Panupong  and
      Liang, Percy",
    editor = "Zong, Chengqing  and
      Strube, Michael",
    booktitle = "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing",
    month = jul,
    year = "2015",
    address = "Beijing, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P15-1142/",
    doi = "10.3115/v1/P15-1142",
    pages = "1470--1480"
}

@article{spider2.024,
  author       = {Fangyu Lei and
                  Jixuan Chen and
                  Yuxiao Ye and
                  Ruisheng Cao and
                  Dongchan Shin and
                  Hongjin Su and
                  Zhaoqing Suo and
                  Hongcheng Gao and
                  Wenjing Hu and
                  Pengcheng Yin and
                  Victor Zhong and
                  Caiming Xiong and
                  Ruoxi Sun and
                  Qian Liu and
                  Sida I. Wang and
                  Tao Yu},
  title        = {{Spider 2.0: Evaluating Language Models on Real-World Enterprise Text-to-SQL
                  Workflows}},
  journal      = {CoRR},
  volume       = {abs/2411.07763},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2411.07763},
  doi          = {10.48550/ARXIV.2411.07763},
  eprinttype    = {arXiv},
  eprint       = {2411.07763},
  timestamp    = {Wed, 01 Jan 2025 11:02:44 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2411-07763.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{Spider18,
    title = "{S}pider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-{SQL} Task",
    author = "Yu, Tao  and
      Zhang, Rui  and
      Yang, Kai  and
      Yasunaga, Michihiro  and
      Wang, Dongxu  and
      Li, Zifan  and
      Ma, James  and
      Li, Irene  and
      Yao, Qingning  and
      Roman, Shanelle  and
      Zhang, Zilin  and
      Radev, Dragomir",
    editor = "Riloff, Ellen  and
      Chiang, David  and
      Hockenmaier, Julia  and
      Tsujii, Jun{'}ichi",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1425/",
    doi = "10.18653/v1/D18-1425",
    pages = "3911--3921",
    abstract = "We present \textit{Spider}, a large-scale complex and cross-domain semantic parsing and text-to-SQL dataset annotated by 11 college students. It consists of 10,181 questions and 5,693 unique complex SQL queries on 200 databases with multiple tables covering 138 different domains. We define a new complex and cross-domain semantic parsing and text-to-SQL task so that different complicated SQL queries and databases appear in train and test sets. In this way, the task requires the model to generalize well to both new SQL queries and new database schemas. Therefore, Spider is distinct from most of the previous semantic parsing tasks because they all use a single database and have the exact same program in the train set and the test set. We experiment with various state-of-the-art models and the best model achieves only 9.7{\%} exact matching accuracy on a database split setting. This shows that Spider presents a strong challenge for future research. Our dataset and task with the most recent updates are publicly available at \url{https://yale-lily.github.io/seq2sql/spider}."
}

@article{wikisql17,
  author       = {Victor Zhong and
                  Caiming Xiong and
                  Richard Socher},
  title        = {{Seq2SQL: Generating Structured Queries from Natural Language using
                  Reinforcement Learning}},
  journal      = {CoRR},
  volume       = {abs/1709.00103},
  year         = {2017},
  url          = {http://arxiv.org/abs/1709.00103},
  eprinttype    = {arXiv},
  eprint       = {1709.00103},
  timestamp    = {Mon, 13 Aug 2018 16:48:41 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1709-00103.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{hybridqa20,
    title = "{H}ybrid{QA}: A Dataset of Multi-Hop Question Answering over Tabular and Textual Data",
    author = "Chen, Wenhu  and
      Zha, Hanwen  and
      Chen, Zhiyu  and
      Xiong, Wenhan  and
      Wang, Hong  and
      Wang, William Yang",
    editor = "Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.91/",
    doi = "10.18653/v1/2020.findings-emnlp.91",
    pages = "1026--1036",
    abstract = "Existing question answering datasets focus on dealing with homogeneous information, based either only on text or KB/Table information alone. However, as human knowledge is distributed over heterogeneous forms, using homogeneous information alone might lead to severe coverage problems. To fill in the gap, we present HybridQA, a new large-scale question-answering dataset that requires reasoning on heterogeneous information. Each question is aligned with a Wikipedia table and multiple free-form corpora linked with the entities in the table. The questions are designed to aggregate both tabular information and text information, i.e., lack of either form would render the question unanswerable. We test with three different models: 1) a table-only model. 2) text-only model. 3) a hybrid model that combines heterogeneous information to find the answer. The experimental results show that the EM scores obtained by two baselines are below 20{\%}, while the hybrid model can achieve an EM over 40{\%}. This gap suggests the necessity to aggregate heterogeneous information in HybridQA. However, the hybrid model`s score is still far behind human performance. Hence, HybridQA can serve as a challenging benchmark to study question answering with heterogeneous information."
}

@article{tablebench24,
  author       = {Xianjie Wu and
                  Jian Yang and
                  Linzheng Chai and
                  Ge Zhang and
                  Jiaheng Liu and
                  Xinrun Du and
                  Di Liang and
                  Daixin Shu and
                  Xianfu Cheng and
                  Tianzhen Sun and
                  Guanglin Niu and
                  Tongliang Li and
                  Zhoujun Li},
  title        = {{TableBench: A Comprehensive and Complex Benchmark for Table Question
                  Answering}},
  journal      = {CoRR},
  volume       = {abs/2408.09174},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2408.09174},
  doi          = {10.48550/ARXIV.2408.09174},
  eprinttype    = {arXiv},
  eprint       = {2408.09174},
  timestamp    = {Tue, 24 Sep 2024 17:36:24 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2408-09174.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{FeTaQA22,
  author       = {Linyong Nan and
                  Chiachun Hsieh and
                  Ziming Mao and
                  Xi Victoria Lin and
                  Neha Verma and
                  Rui Zhang and
                  Wojciech Kryscinski and
                  Hailey Schoelkopf and
                  Riley Kong and
                  Xiangru Tang and
                  Mutethia Mutuma and
                  Ben Rosand and
                  Isabel Trindade and
                  Renusree Bandaru and
                  Jacob Cunningham and
                  Caiming Xiong and
                  Dragomir R. Radev},
  title        = {{FeTaQA: Free-form Table Question Answering}},
  journal      = {Trans. Assoc. Comput. Linguistics},
  volume       = {10},
  pages        = {35--49},
  year         = {2022},
  url          = {https://doi.org/10.1162/tacl\_a\_00446},
  doi          = {10.1162/TACL\_A\_00446},
  timestamp    = {Wed, 19 Jun 2024 17:28:03 +0200},
  biburl       = {https://dblp.org/rec/journals/tacl/NanHMLVZKSKTMRT22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{TableLlama23,
  author       = {Tianshu Zhang and
                  Xiang Yue and
                  Yifei Li and
                  Huan Sun},
  title        = {{TableLlama: Towards Open Large Generalist Models for Tables}},
  journal      = {CoRR},
  volume       = {abs/2311.09206},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2311.09206},
  doi          = {10.48550/ARXIV.2311.09206},
  eprinttype    = {arXiv},
  eprint       = {2311.09206},
  timestamp    = {Mon, 03 Mar 2025 21:33:30 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2311-09206.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{Table-GPT24,
author = {Li, Peng and He, Yeye and Yashar, Dror and Cui, Weiwei and Ge, Song and Zhang, Haidong and Rifinski Fainman, Danielle and Zhang, Dongmei and Chaudhuri, Surajit},
title = {{Table-GPT: Table Fine-tuned GPT for Diverse Table Tasks}},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
url = {https://doi.org/10.1145/3654979},
doi = {10.1145/3654979},
abstract = {Language models, such as GPT-3 and ChatGPT, demonstrate remarkable abilities to follow diverse human instructions and perform a wide range of tasks, using instruction fine-tuning. However, when we test language models with a range of basic table-understanding tasks, we observe that today's language models are still sub-optimal in many table-related tasks, likely because they are pre-trained predominantly on one-dimensional natural-language texts, whereas relational tables are two-dimensional objects. In this work, we propose a new "emphtable fine-tuning '' paradigm, where we continue to train/fine-tune language models like GPT-3.5 and ChatGPT, using diverse table-tasks synthesized from real tables as training data, which is analogous to "instruction fine-tuning'', but with the goal of enhancing language models' ability to understand tables and perform table tasks. We show that our resulting sys models demonstrate: (1) better table-understanding capabilities, by consistently outperforming the vanilla GPT-3.5 and ChatGPT, on a wide range of table tasks (data transformation, data cleaning, data profiling, data imputation, table-QA, etc.), including tasks that are completely holdout and unseen during training, and (2) strong generalizability, in its ability to respond to diverse human instructions to perform new and unseen table-tasks, in a manner similar to GPT-3.5 and ChatGPT. Our code and data have been released at https://github.com/microsoft/Table-GPT for future research.},
journal = {Proc. ACM Manag. Data},
month = may,
articleno = {176},
numpages = {28},
keywords = {instruction fine-tuning, language models, model generalizability, multi-task training, synthesized training data, table fine-tuning, table models, table tasks, unseen tasks}
}



@article{BonfittoCM21,
  author       = {Sara Bonfitto and
                  Elena Casiraghi and
                  Marco Mesiti},
  title        = {{Table understanding approaches for extracting knowledge from heterogeneous
                  tables}},
  journal      = {WIREs Data Mining Knowl. Discov.},
  volume       = {11},
  number       = {4},
  year         = {2021},
  url          = {https://doi.org/10.1002/widm.1407},
  doi          = {10.1002/WIDM.1407},
  timestamp    = {Fri, 21 Jan 2022 14:37:57 +0100},
  biburl       = {https://dblp.org/rec/journals/widm/BonfittoCM21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{TableFormer22,
  author       = {Ahmed S. Nassar and
                  Nikolaos Livathinos and
                  Maksym Lysak and
                  Peter W. J. Staar},
  title        = {{TableFormer: Table Structure Understanding with Transformers}},
  journal      = {CoRR},
  volume       = {abs/2203.01017},
  year         = {2022},
  url          = {https://doi.org/10.48550/arXiv.2203.01017},
  doi          = {10.48550/ARXIV.2203.01017},
  eprinttype    = {arXiv},
  eprint       = {2203.01017},
  timestamp    = {Mon, 13 Jun 2022 11:17:30 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2203-01017.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{Llava,
  author       = {Haotian Liu and
                  Chunyuan Li and
                  Yuheng Li and
                  Yong Jae Lee},
  title        = {{Improved Baselines with Visual Instruction Tuning}},
  booktitle    = {{IEEE/CVF} Conference on Computer Vision and Pattern Recognition,
                  {CVPR} 2024, Seattle.},
  pages        = {26286--26296},
  publisher    = {{IEEE}},
  year         = {2024},
  url          = {https://doi.org/10.1109/CVPR52733.2024.02484},
  doi          = {10.1109/CVPR52733.2024.02484},
  timestamp    = {Sun, 19 Jan 2025 13:39:05 +0100},
  biburl       = {https://dblp.org/rec/conf/cvpr/LiuLLL24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}

}

@inproceedings{wu2025mmqa,
  author       = {Jian Wu and
                  Linyi Yang and
                  Dongyuan Li and
                  Yuliang Ji and
                  Manabu Okumura and
                  Yue Zhang},
  title        = {{MMQA:} Evaluating LLMs with Multi-Table Multi-Hop Complex Questions},
  booktitle    = {The Thirteenth International Conference on Learning Representations,
                  {ICLR} 2025, Singapore},
  publisher    = {OpenReview.net},
  year         = {2025},
  url          = {https://openreview.net/forum?id=GGlpykXDCa},
  timestamp    = {Thu, 15 May 2025 17:19:05 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/WuYLJOZ25.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{Junnan2023,
  author       = {Junnan Li and
                  Dongxu Li and
                  Silvio Savarese and
                  Steven C. H. Hoi},
  title        = {{BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image
                  Encoders and Large Language Models}},
  journal      = {CoRR},
  volume       = {abs/2301.12597},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2301.12597},
  doi          = {10.48550/ARXIV.2301.12597},
  eprinttype    = {arXiv},
  eprint       = {2301.12597},
  timestamp    = {Wed, 01 Feb 2023 14:38:31 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2301-12597.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{2023GPT4VisionSC,
  title={{GPT-4V(ision) System Card}},
  author={{OpenAI}},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:263218031}
}


@inproceedings{multiTabQA23,
    title = "{M}ulti{T}ab{QA}: Generating Tabular Answers for Multi-Table Question Answering",
    author = "Pal, Vaishali  and
      Yates, Andrew  and
      Kanoulas, Evangelos  and
      de Rijke, Maarten",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.348/",
    doi = "10.18653/v1/2023.acl-long.348",
    pages = "6322--6334",
    abstract = "Recent advances in tabular question answering (QA) with large language models are constrained in their coverage and only answer questions over a single table. However, real-world queries are complex in nature, often over multiple tables in a relational database or web page. Single table questions do not involve common table operations such as set operations, Cartesian products (joins), or nested queries. Furthermore, multi-table operations often result in a tabular output, which necessitates table generation capabilities of tabular QA models. To fill this gap, we propose a new task of answering questions over multiple tables. Our model, MultiTabQA, not only answers questions over multiple tables, but also generalizes to generate tabular answers. To enable effective training, we build a pre-training dataset comprising of 132,645 SQL queries and tabular answers. Further, we evaluate the generated tables by introducing table-specific metrics of varying strictness assessing various levels of granularity of the table structure. MultiTabQA outperforms state-of-the-art single table QA models adapted to a multi-table QA setting by finetuning on three datasets: Spider, Atis and GeoQuery."
}
@article{qwen2.5vl,
  author       = {An Yang and
                  Baosong Yang and
                  Beichen Zhang and
                  Binyuan Hui and
                  Bo Zheng and
                  Bowen Yu and
                  Chengyuan Li and
                  Dayiheng Liu and
                  Fei Huang and
                  Haoran Wei and
                  Huan Lin and
                  Jian Yang and
                  Jianhong Tu and
                  Jianwei Zhang and
                  Jianxin Yang and
                  Jiaxi Yang and
                  Jingren Zhou and
                  Junyang Lin and
                  Kai Dang and
                  Keming Lu and
                  Keqin Bao and
                  Kexin Yang and
                  Le Yu and
                  Mei Li and
                  Mingfeng Xue and
                  Pei Zhang and
                  Qin Zhu and
                  Rui Men and
                  Runji Lin and
                  Tianhao Li and
                  Tingyu Xia and
                  Xingzhang Ren and
                  Xuancheng Ren and
                  Yang Fan and
                  Yang Su and
                  Yichang Zhang and
                  Yu Wan and
                  Yuqiong Liu and
                  Zeyu Cui and
                  Zhenru Zhang and
                  Zihan Qiu},
  title        = {{Qwen2.5 Technical Report}},
  journal      = {CoRR},
  volume       = {abs/2412.15115},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2412.15115},
  doi          = {10.48550/ARXIV.2412.15115},
  eprinttype    = {arXiv},
  eprint       = {2412.15115},
  timestamp    = {Wed, 19 Mar 2025 21:16:34 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2412-15115.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{llava-ov,
  author       = {Bo Li and
                  Yuanhan Zhang and
                  Dong Guo and
                  Renrui Zhang and
                  Feng Li and
                  Hao Zhang and
                  Kaichen Zhang and
                  Yanwei Li and
                  Ziwei Liu and
                  Chunyuan Li},
  title        = {{LLaVA-OneVision: Easy Visual Task Transfer}},
  journal      = {CoRR},
  volume       = {abs/2408.03326},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2408.03326},
  doi          = {10.48550/ARXIV.2408.03326},
  eprinttype    = {arXiv},
  eprint       = {2408.03326},
  timestamp    = {Thu, 12 Sep 2024 21:06:52 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2408-03326.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc {internvl3,
      title={{InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models}}, 
      author= {Jinguo Zhu and Weiyun Wang and Zhe Chen and Zhaoyang Liu and Shenglong Ye and Lixin Gu and Hao Tian and Yuchen Duan and Weijie Su and Jie Shao and Zhangwei Gao and Erfei Cui and Xuehui Wang and Yue Cao and Yangzhou Liu and Xingguang Wei and Hongjie Zhang and Haomin Wang and Weiye Xu and Hao Li and Jiahao Wang and Nianchen Deng and Songze Li and Yinan He and Tan Jiang and Jiapeng Luo and Yi Wang and Conghui He and Botian Shi and Xingcheng Zhang and Wenqi Shao and Junjun He and Yingtong Xiong and Wenwen Qu and Peng Sun and Penglong Jiao and Han Lv and Lijun Wu and Kaipeng Zhang and Huipeng Deng and Jiaye Ge and Kai Chen and Limin Wang and Min Dou and Lewei Lu and Xizhou Zhu and Tong Lu and Dahua Lin and Yu Qiao and Jifeng Dai and Wenhai Wang},
      year={2025},
      eprint={2504.10479},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2504.10479}, 
}


@article{phi3,
  author       = {Marah I Abdin and
                  Sam Ade Jacobs and
                  Ammar Ahmad Awan and
                  Jyoti Aneja and
                  Ahmed Awadallah and
                  Hany Awadalla and
                  Nguyen Bach and
                  Amit Bahree and
                  Arash Bakhtiari and
                  Harkirat S. Behl and
                  Alon Benhaim and
                  Misha Bilenko and
                  Johan Bjorck and
                  S{\'{e}}bastien Bubeck and
                  Martin Cai and
                  Caio C{\'{e}}sar Teodoro Mendes and
                  Weizhu Chen and
                  Vishrav Chaudhary and
                  Parul Chopra and
                  Allie Del Giorno and
                  Gustavo de Rosa and
                  Matthew Dixon and
                  Ronen Eldan and
                  Dan Iter and
                  Amit Garg and
                  Abhishek Goswami and
                  Suriya Gunasekar and
                  Emman Haider and
                  Junheng Hao and
                  Russell J. Hewett and
                  Jamie Huynh and
                  Mojan Javaheripi and
                  Xin Jin and
                  Piero Kauffmann and
                  Nikos Karampatziakis and
                  Dongwoo Kim and
                  Mahoud Khademi and
                  Lev Kurilenko and
                  James R. Lee and
                  Yin Tat Lee and
                  Yuanzhi Li and
                  Chen Liang and
                  Weishung Liu and
                  Eric Lin and
                  Zeqi Lin and
                  Piyush Madan and
                  Arindam Mitra and
                  Hardik Modi and
                  Anh Nguyen and
                  Brandon Norick and
                  Barun Patra and
                  Daniel Perez{-}Becker and
                  Thomas Portet and
                  Reid Pryzant and
                  Heyang Qin and
                  Marko Radmilac and
                  Corby Rosset and
                  Sambudha Roy and
                  Olatunji Ruwase and
                  Olli Saarikivi and
                  Amin Saied and
                  Adil Salim and
                  Michael Santacroce and
                  Shital Shah and
                  Ning Shang and
                  Hiteshi Sharma and
                  Xia Song and
                  Masahiro Tanaka and
                  Xin Wang and
                  Rachel Ward and
                  Guanhua Wang and
                  Philipp Witte and
                  Michael Wyatt and
                  Can Xu and
                  Jiahang Xu and
                  Sonali Yadav and
                  Fan Yang and
                  Ziyi Yang and
                  Donghan Yu and
                  Chengruidong Zhang and
                  Cyril Zhang and
                  Jianwen Zhang and
                  Li Lyna Zhang and
                  Yi Zhang and
                  Yue Zhang and
                  Yunan Zhang and
                  Xiren Zhou},
  title        = {{Phi-3 Technical Report: A Highly Capable Language Model Locally
                  on Your Phone}},
  journal      = {CoRR},
  volume       = {abs/2404.14219},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2404.14219},
  doi          = {10.48550/ARXIV.2404.14219},
  eprinttype    = {arXiv},
  eprint       = {2404.14219},
  timestamp    = {Mon, 16 Sep 2024 12:58:56 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2404-14219.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{internvl,
    title={{Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks}},
    author={Chen, Zhe and Wu, Jiannan and Wang, Wenhai and Su, Weijie and Chen, Guo and Xing, Sen and Zhong, Muyan and Zhang, Qinglong and Zhu, Xizhou and Lu, Lewei and others},
    booktitle={Proceedings of the {IEEE/CVF} Conference on Computer Vision and Pattern Recognition},
    pages={24185--24198},
    year={2024}
  }
@article{gemma3,
  author       = {Aishwarya Kamath and
                  Johan Ferret and
                  Shreya Pathak and
                  Nino Vieillard and
                  Ramona Merhej and
                  Sarah Perrin and
                  Tatiana Matejovicova and
                  Alexandre Ram{\'{e}} and
                  Morgane Rivi{\`{e}}re and
                  Louis Rouillard and
                  Thomas Mesnard and
                  Geoffrey Cideron and
                  Jean{-}Bastien Grill and
                  Sabela Ramos and
                  Edouard Yvinec and
                  Michelle Casbon and
                  Etienne Pot and
                  Ivo Penchev and
                  Ga{\"{e}}l Liu and
                  Francesco Visin and
                  Kathleen Kenealy and
                  Lucas Beyer and
                  Xiaohai Zhai and
                  Anton Tsitsulin and
                  R{\'{o}}bert Busa{-}Fekete and
                  Alex Feng and
                  Noveen Sachdeva and
                  Benjamin Coleman and
                  Yi Gao and
                  Basil Mustafa and
                  Iain Barr and
                  Emilio Parisotto and
                  David Tian and
                  Matan Eyal and
                  Colin Cherry and
                  Jan{-}Thorsten Peter and
                  Danila Sinopalnikov and
                  Surya Bhupatiraju and
                  Rishabh Agarwal and
                  Mehran Kazemi and
                  Dan Malkin and
                  Ravin Kumar and
                  David Vilar and
                  Idan Brusilovsky and
                  Jiaming Luo and
                  Andreas Steiner and
                  Abe Friesen and
                  Abhanshu Sharma and
                  Abheesht Sharma and
                  Adi Mayrav Gilady and
                  Adrian Goedeckemeyer and
                  Alaa Saade and
                  Alexander Kolesnikov and
                  Alexei Bendebury and
                  Alvin Abdagic and
                  Amit Vadi and
                  Andr{\'{a}}s Gy{\"{o}}rgy and
                  Andr{\'{e}} Susano Pinto and
                  Anil Das and
                  Ankur Bapna and
                  Antoine Miech and
                  Antoine Yang and
                  Antonia Paterson and
                  Ashish Shenoy and
                  Ayan Chakrabarti and
                  Bilal Piot and
                  Bo Wu and
                  Bobak Shahriari and
                  Bryce Petrini and
                  Charlie Chen and
                  Charline Le Lan and
                  Christopher A. Choquette{-}Choo and
                  CJ Carey and
                  Cormac Brick and
                  Daniel Deutsch and
                  Danielle Eisenbud and
                  Dee Cattle and
                  Derek Cheng and
                  Dimitris Paparas and
                  Divyashree Shivakumar Sreepathihalli and
                  Doug Reid and
                  Dustin Tran and
                  Dustin Zelle and
                  Eric Noland and
                  Erwin Huizenga and
                  Eugene Kharitonov and
                  Frederick Liu and
                  Gagik Amirkhanyan and
                  Glenn Cameron and
                  Hadi Hashemi and
                  Hanna Klimczak{-}Plucinska and
                  Harman Singh and
                  Harsh Mehta and
                  Harshal Tushar Lehri and
                  Hussein Hazimeh and
                  Ian Ballantyne and
                  Idan Szpektor and
                  Ivan Nardini},
  title        = {{Gemma 3 Technical Report}},
  journal      = {CoRR},
  volume       = {abs/2503.19786},
  year         = {2025},
  url          = {https://doi.org/10.48550/arXiv.2503.19786},
  doi          = {10.48550/ARXIV.2503.19786},
  eprinttype    = {arXiv},
  eprint       = {2503.19786},
  timestamp    = {Mon, 21 Apr 2025 14:23:16 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2503-19786.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{LoRA,
  author       = {Edward J. Hu and
                  Yelong Shen and
                  Phillip Wallis and
                  Zeyuan Allen{-}Zhu and
                  Yuanzhi Li and
                  Shean Wang and
                  Weizhu Chen},
  title        = {{LoRA: Low-Rank Adaptation of Large Language Models}},
  journal      = {CoRR},
  volume       = {abs/2106.09685},
  year         = {2021},
  url          = {https://arxiv.org/abs/2106.09685},
  eprinttype    = {arXiv},
  eprint       = {2106.09685},
  timestamp    = {Tue, 29 Jun 2021 16:55:04 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2106-09685.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{post_training,
  author       = {Guiyao Tie and
                  Zeli Zhao and
                  Dingjie Song and
                  Fuyang Wei and
                  Rong Zhou and
                  Yurou Dai and
                  Wen Yin and
                  Zhejian Yang and
                  Jiangyue Yan and
                  Yao Su and
                  Zhenhan Dai and
                  Yifeng Xie and
                  Yihan Cao and
                  Lichao Sun and
                  Pan Zhou and
                  Lifang He and
                  Hechang Chen and
                  Yu Zhang and
                  Qingsong Wen and
                  Tianming Liu and
                  Neil Zhenqiang Gong and
                  Jiliang Tang and
                  Caiming Xiong and
                  Heng Ji and
                  Philip S. Yu and
                  Jianfeng Gao},
  title        = {{A Survey on Post-training of Large Language Models}},
  journal      = {CoRR},
  volume       = {abs/2503.06072},
  year         = {2025},
  url          = {https://doi.org/10.48550/arXiv.2503.06072},
  doi          = {10.48550/ARXIV.2503.06072},
  eprinttype    = {arXiv},
  eprint       = {2503.06072},
  timestamp    = {Mon, 14 Apr 2025 08:08:44 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2503-06072.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{deepseekR1,
  author       = {{DeepSeek AI} and  % Corrected author for organizations
                  Daya Guo and
                  Dejian Yang and
                  Haowei Zhang and
                  Junxiao Song and
                  Ruoyu Zhang and
                  Runxin Xu and
                  Qihao Zhu and
                  Shirong Ma and
                  Peiyi Wang and
                  Xiao Bi and
                  Xiaokang Zhang and
                  Xingkai Yu and
                  Yu Wu and
                  Z. F. Wu and
                  Zhibin Gou and
                  Zhihong Shao and
                  Zhuoshu Li and
                  Ziyi Gao and
                  Aixin Liu and
                  Bing Xue and
                  Bingxuan Wang and
                  Bochao Wu and
                  Bei Feng and
                  Chengda Lu and
                  Chenggang Zhao and
                  Chengqi Deng and
                  Chenyu Zhang and
                  Chong Ruan and
                  Damai Dai and
                  Deli Chen and
                  Dongjie Ji and
                  Erhang Li and
                  Fangyun Lin and
                  Fucong Dai and
                  Fuli Luo and
                  Guangbo Hao and
                  Guanting Chen and
                  Guowei Li and
                  H. Zhang and
                  Han Bao and
                  Hanwei Xu and
                  Haocheng Wang and
                  Honghui Ding and
                  Huajian Xin and
                  Huazuo Gao and
                  Hui Qu and
                  Hui Li and
                  Jianzhong Guo and
                  Jiashi Li and
                  Jiawei Wang and
                  Jingchang Chen and
                  Jingyang Yuan and
                  Junjie Qiu and
                  Junlong Li and
                  J. L. Cai and
                  Jiaqi Ni and
                  Jian Liang and
                  Jin Chen and
                  Kai Dong and
                  Kai Hu and
                  Kaige Gao and
                  Kang Guan and
                  Kexin Huang and
                  Kuai Yu and
                  Lean Wang and
                  Lecong Zhang and
                  Liang Zhao and
                  Litong Wang and
                  Liyue Zhang and
                  Lei Xu and
                  Leyi Xia and
                  Mingchuan Zhang and
                  Minghua Zhang and
                  Minghui Tang and
                  Meng Li and
                  Miaojun Wang and
                  Mingming Li and
                  Ning Tian and
                  Panpan Huang and
                  Peng Zhang and
                  Qiancheng Wang and
                  Qinyu Chen and
                  Qiushi Du and
                  Ruiqi Ge and
                  Ruisong Zhang and
                  Ruizhe Pan and
                  Runji Wang and
                  R. J. Chen and
                  R. L. Jin and
                  Ruyi Chen and
                  Shanghao Lu and
                  Shangyan Zhou and
                  Shanhuang Chen and
                  Shengfeng Ye and
                  Shiyu Wang and
                  Shuiping Yu and
                  Shunfeng Zhou and
                  Shuting Pan and
                  S. S. Li},
  title        = {{DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement
                  Learning}},
  journal      = {CoRR},
  volume       = {abs/2501.12948},
  year         = {2025},
  url          = {https://doi.org/10.48550/arXiv.2501.12948},
  doi          = {10.48550/ARXIV.2501.12948},
  eprinttype    = {arXiv},
  eprint       = {2501.12948},
  timestamp    = {Tue, 25 Feb 2025 13:58:33 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2501-12948.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{deepseekmath,
  author       = {Zhihong Shao and
                  Peiyi Wang and
                  Qihao Zhu and
                  Runxin Xu and
                  Junxiao Song and
                  Mingchuan Zhang and
                  Y. K. Li and
                  Y. Wu and
                  Daya Guo},
  title        = {DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open
                  Language Models},
  journal      = {CoRR},
  volume       = {abs/2402.03300},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2402.03300},
  doi          = {10.48550/ARXIV.2402.03300},
  eprinttype    = {arXiv},
  eprint       = {2402.03300},
  timestamp    = {Mon, 12 Feb 2024 13:36:38 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2402-03300.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

